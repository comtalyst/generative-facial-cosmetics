{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Environment Setup**\n",
    "Run all when initiating session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve config file, if using colab (nothing should happen if you use Windows)\n",
    "!cp './drive/My Drive/Live Workspace/generative-facial-cosmetics/encoder_engineering/config.py' '.'\n",
    "!mkdir data\n",
    "!cp -a './drive/My Drive/Live Workspace/generative-facial-cosmetics/encoder_engineering/data/' '.'\n",
    "!mkdir models\n",
    "!cp -a './drive/My Drive/Live Workspace/generative-facial-cosmetics/encoder_engineering/models/' '.'\n",
    "!mkdir technical\n",
    "!cp -a './drive/My Drive/Live Workspace/generative-facial-cosmetics/encoder_engineering/technical/' '.'\n",
    "!mkdir utils\n",
    "!cp -a './drive/My Drive/Live Workspace/generative-facial-cosmetics/encoder_engineering/utils/' '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *                            # config.py\n",
    "from matplotlib import pyplot as plt \n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GCS Integration**\n",
    "Run all when initiating session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isWindows():\n",
    "  import utils.gcs_windows as gcs \n",
    "elif isColab():\n",
    "  import utils.gcs_colab as gcs\n",
    "else:\n",
    "  raise NotImplementedError('OS is not supported yet')\n",
    "\n",
    "if isColab():\n",
    "  gcs.init()\n",
    "# no need of buckets for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### tf-side debug ###\n",
    "# more info: https://github.com/tensorflow/tensorflow/issues/29931\n",
    "import tensorflow as tf\n",
    "temp = tf.zeros([2, 16, 16, 3])  # Or tf.zeros\n",
    "tf.keras.applications.vgg16.preprocess_input(temp)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data.pipeline import *\n",
    "from technical.accelerators import strategy\n",
    "from utils.generator_loading_utils import load_generator_checkpoint\n",
    "from utils.encoder_loading_utils import load_encoder_checkpoint\n",
    "from utils.face_utils import detect_and_crop_lips, replace_lips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "##### Create and load saved generator here (checkpoint) #####\n",
    "# please put generator's saved checkpoint in OUTPUT/generator_checkpoints (GCS for colab)\n",
    "RUN = True\n",
    "if RUN:\n",
    "  generator = load_generator_checkpoint(strategy, model_type='256')\n",
    "  print(generator)\n",
    "else:\n",
    "  print(\"Running switch for this cell is off, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Create and load saved encoder here (checkpoint) #####\n",
    "# please put encoder's saved checkpoint in OUTPUT/encoder_checkpoints (GCS for colab)\n",
    "RUN = True\n",
    "if RUN:\n",
    "  encoder = load_encoder_checkpoint(strategy)\n",
    "  print(encoder)\n",
    "else:\n",
    "  print(\"Running switch for this cell is off, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LATENT_SIZE = encoder.model.layers[-1].output.shape[1:] \n",
    "IMAGE_SHAPE = encoder.model.layers[0].output.shape[1:] \n",
    "IMAGE_SIZE = [IMAGE_SHAPE[0], IMAGE_SHAPE[0]]\n",
    "print(\"Latent size: \" + str(LATENT_SIZE))\n",
    "print(\"Image shape: \" + str(IMAGE_SHAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input\n",
    "img_name = 'test_2.png'\n",
    "img_path = os.path.join(DIR, os.path.join('samples', img_name))\n",
    "img = load_image(img_path)\n",
    "plt.imshow(img[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## crop\n",
    "cropped_lips, p_data, img = detect_and_crop_lips(img_full=img)\n",
    "cropped_lips = np.array(cropped_lips)/255\n",
    "#plt.imshow(cropped_lips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cropped_lips #cv2.resize(cropped_lips, dsize=(90, 90), interpolation=cv2.INTER_CUBIC)\n",
    "plt.imshow(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Encode the cropped lips\n",
    "encoded = encoder.model(np.expand_dims(resized, 0))\n",
    "#print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Regeneration (w/o injections)\n",
    "regenerated = generator.model(encoded, training=False)[0]\n",
    "plt.imshow(regenerated)\n",
    "print(\"Visual loss: \" + str(float(tf.keras.losses.MSE(tf.keras.backend.flatten(resized), tf.keras.backend.flatten(regenerated)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Replacement\n",
    "#img = cv2.resize(img, dsize=(90, 90), interpolation=cv2.INTER_CUBIC)\n",
    "new_img = replace_lips(regenerated.numpy(), p_data, img_full=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injectible = generator.get_injectible_model(strategy)\n",
    "inputs = injectible.inputs\n",
    "for layer in inputs:\n",
    "  print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load to-be-injected image\n",
    "img2_name = 'purple_lips.png'\n",
    "img2_path = os.path.join(DIR, os.path.join('samples', img2_name))\n",
    "img2 = load_image(img2_path)\n",
    "plt.imshow(img2[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crop to-be-injected image\n",
    "cropped_lips2, p_data2, img2 = detect_and_crop_lips(img_full=img2)\n",
    "cropped_lips2 = np.array(cropped_lips2)/255\n",
    "plt.imshow(cropped_lips2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encode the cropped lips\n",
    "encoded2 = encoder.model(np.expand_dims(cropped_lips2, 0))\n",
    "#print(encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regeneration (w/ injections)\n",
    "regenerated_mix = injectible([encoded]*6 + [encoded2*2], training=False)[0]\n",
    "plt.imshow(regenerated_mix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replacement\n",
    "new_img = replace_lips(regenerated_mix.numpy(), p_data, img_full=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imdecode(np.fromstring(cv2.imencode(\".png\", img)[1].tostring(), np.uint8), cv2.IMREAD_COLOR)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regeneration (w/o injections)\n",
    "regenerated = injectible([encoded2]*6 + [encoded2*2], training=False)[0]\n",
    "plt.imshow(regenerated)\n",
    "print(\"Visual loss: \" + str(float(tf.keras.losses.MSE(tf.keras.backend.flatten(cropped_lips2), tf.keras.backend.flatten(regenerated)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experiments**\n",
    "Just a playground for trying out codes, nothing related at all, do not execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGBA))\n",
    "plt.imshow(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_x, min_y, max_x, max_y, offset_x, offset_y = p_data\n",
    "cropped_lips_pil = Image.fromarray((cropped_lips*255).astype('uint8'))\n",
    "cropped_lips_pil = cropped_lips_pil.crop((offset_x, offset_y, offset_x+(max_x-min_x), offset_y+(max_y-min_y)))\n",
    "img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGBA))\n",
    "img_pil.paste(cropped_lips_pil, (min_x, min_y), cropped_lips_pil)\n",
    "plt.imshow(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_x, min_y, max_x, max_y, offset_x, offset_y = p_data\n",
    "img_tmp = (cropped_lips*255).astype('uint8')\n",
    "cropped_lips_pil = Image.fromarray(img_tmp)\n",
    "cropped_lips_pil = cropped_lips_pil.crop((offset_x, offset_y, offset_x+(max_x-min_x), offset_y+(max_y-min_y)))\n",
    "plt.imshow(cropped_lips_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED encode cv2 image to str rep. of bytes\n",
    "with open(\"encoded_tmp.txt\", \"w\") as f:\n",
    "    f.write(str(cv2.imencode(\".png\", img)[1].tostring()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEPRECATED retrieve the str rep. of bytes and convert back to cv2\n",
    "with open(\"encoded_tmp.txt\", \"r\") as f:\n",
    "    fstr = f.read()\n",
    "fbytes = eval(fstr)\n",
    "plt.imshow(cv2.imdecode(np.fromstring(fbytes, np.uint8), cv2.IMREAD_COLOR)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new method (more stable storage)\n",
    "import base64\n",
    "orig_bytes = cv2.imencode(\".png\", img)[1].tostring()\n",
    "pre_transportable = base64.b64encode(orig_bytes)\n",
    "transportable = str(pre_transportable, \"ANSI\")\n",
    "post_transportable = bytes(transportable, \"ANSI\")\n",
    "bbytes = base64.b64decode(post_transportable)\n",
    "plt.imshow(cv2.imdecode(np.fromstring(bbytes, np.uint8), cv2.IMREAD_COLOR)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_tmp.txt\", \"w\") as f:\n",
    "    f.write(transportable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_tmp.txt\", \"r\") as f:\n",
    "    transportable = f.read()\n",
    "post_transportable = bytes(transportable, \"ANSI\")\n",
    "bbytes = base64.b64decode(post_transportable)\n",
    "plt.imshow(cv2.imdecode(np.fromstring(bbytes, np.uint8), cv2.IMREAD_COLOR)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaIN(x):\n",
    "    # Normalize x[0] (image representation)\n",
    "    mean = tf.keras.backend.mean(x[0], axis = [1, 2], keepdims = True)\n",
    "    std = tf.keras.backend.std(x[0], axis = [1, 2], keepdims = True) + 1e-7\n",
    "    y = (x[0] - mean) / std\n",
    "    \n",
    "    # Reshape scale and bias parameters\n",
    "    pool_shape = [-1, 1, 1, y.shape[-1]]\n",
    "    scale = tf.keras.backend.reshape(x[1], pool_shape)\n",
    "    bias = tf.keras.backend.reshape(x[2], pool_shape)\n",
    "    \n",
    "    # Multiply by x[1] (GAMMA) and add x[2] (BETA)\n",
    "    return y * scale + bias\n",
    "\n",
    "tmp_model = tf.keras.models.load_model(\"./outputs/saved_models/current/generator-p_5-e_9999.h5\", custom_objects={'AdaIN': AdaIN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regeneration (w/o injections)\n",
    "regenerated = tmp_model(encoded, training=False)[0]\n",
    "plt.imshow(regenerated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_bytes = cv2.imencode(\".png\", np.array(new_img))[1].tostring()\n",
    "pre_transportable = base64.b64encode(orig_bytes)\n",
    "transportable = str(pre_transportable, \"ANSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6eb4991bf65e38b0cae10071082bf7d19677db44169e53c1726170efc336e1b8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}